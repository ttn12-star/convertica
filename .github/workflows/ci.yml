name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    tags:
      - 'v*'  # Deploy on version tags (e.g., v1.0.0)
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'

jobs:
  # Lint and Code Quality
  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy
          pip install -r requirements.txt

      - name: Run flake8
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Check code formatting with black
        run: |
          black --check . || (echo "âš ï¸ Code formatting issues found. Run 'black .' to fix." && exit 1)

      - name: Check import sorting with isort
        run: |
          isort --check-only --profile black . || (echo "âš ï¸ Import sorting issues found. Run 'isort --profile black .' to fix." && exit 1)

  # Security Scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install safety bandit
          pip install -r requirements.txt

      - name: Run Safety check
        run: safety check --json || true

      - name: Run Bandit security scan
        run: bandit -r . -f json -o bandit-report.json || true

  # Tests
  test:
    name: Run Tests
    runs-on: ubuntu-latest

    # Centralized database configuration to ensure consistency
    env:
      POSTGRES_DB: convertica_test
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      DATABASE_ENGINE: postgresql
      DATABASE_NAME: convertica_test
      DATABASE_USER: postgres
      DATABASE_PASSWORD: postgres
      DATABASE_HOST: localhost
      DATABASE_PORT: 5432
      SECRET_KEY: test-secret-key
      DEBUG: True
      REDIS_URL: redis://localhost:6379/1
      CELERY_BROKER_URL: redis://localhost:6379/0
      CELERY_RESULT_BACKEND: redis://localhost:6379/0

    services:
      postgres:
        image: postgres:16
        env:
          # PostgreSQL container uses POSTGRES_* variables
          # Values must match DATABASE_* variables in job env above
          POSTGRES_DB: convertica_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install coverage

      - name: Run migrations
        # DATABASE_* variables are inherited from job-level env
        run: |
          python manage.py migrate

      - name: Run tests with coverage
        # DATABASE_* and other env variables are inherited from job-level env
        # Using Django's manage.py test (not pytest) as it's the project's test runner
        run: |
          coverage run --source='utils_site' manage.py test --verbosity=2
          coverage xml -o coverage.xml --ignore-errors
          coverage report --ignore-errors

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v5
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  # Build Docker Image
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push'
    # Cancel in-progress builds when a new push/tag arrives
    # For tags: use common group so new tags cancel old tag builds
    # For branches: use branch-specific group so branches don't cancel each other
    concurrency:
      group: ${{ startsWith(github.ref, 'refs/tags/') && 'build-tags' || format('build-{0}', github.ref) }}
      cancel-in-progress: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            image=moby/buildkit:latest

      - name: Login to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
          logout: false

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.DOCKER_USERNAME }}/convertica
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix=sha-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./ci/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=build
          cache-to: type=gha,mode=max,scope=build
          # Build only for amd64 to speed up (uncomment arm64 if needed)
          platforms: linux/amd64

  # Deploy to Production Server
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build]
    # Deploy ONLY on version tags (v*), NOT on push to main
    # This ensures deployment happens only after creating a release tag
    if: |
      startsWith(github.ref, 'refs/tags/v') && github.event_name == 'push'
    # Cancel in-progress deployments when a new tag is pushed
    # This prevents wasting resources on outdated deployments
    concurrency:
      group: deploy-production
      cancel-in-progress: true
    environment:
      name: production
      url: https://convertica.net
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history to compare tags

      - name: Check if newer tag exists
        id: check_newer_tag
        run: |
          CURRENT_TAG="${GITHUB_REF#refs/tags/}"
          echo "Current tag: $CURRENT_TAG"

          # Fetch all tags
          git fetch --tags --force

          # Get all version tags sorted by version
          ALL_TAGS=$(git tag -l "v*" | sort -V)

          # Find the latest tag
          LATEST_TAG=$(echo "$ALL_TAGS" | tail -1)
          echo "Latest tag: $LATEST_TAG"

          # Compare versions using sort -V (version sort)
          # If current tag is not the latest, cancel deployment
          if [ "$CURRENT_TAG" != "$LATEST_TAG" ]; then
            # Check if current tag is older than latest using version sort
            SORTED_TAGS=$(printf '%s\n' "$CURRENT_TAG" "$LATEST_TAG" | sort -V)
            FIRST_TAG=$(echo "$SORTED_TAGS" | head -1)

            if [ "$FIRST_TAG" != "$CURRENT_TAG" ]; then
              echo "âš ï¸ Newer tag $LATEST_TAG exists, current tag $CURRENT_TAG is outdated"
              echo "ðŸ›‘ Cancelling deployment to avoid deploying outdated version"
              echo "should_cancel=true" >> $GITHUB_OUTPUT
            else
              echo "âœ… Current tag $CURRENT_TAG is newer than $LATEST_TAG (should not happen, but proceeding)"
              echo "should_cancel=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "âœ… Current tag $CURRENT_TAG is the latest, proceeding with deployment"
            echo "should_cancel=false" >> $GITHUB_OUTPUT
          fi

      - name: Cancel deployment if newer tag exists
        if: steps.check_newer_tag.outputs.should_cancel == 'true'
        run: |
          echo "ðŸ›‘ Cancelling deployment - newer tag exists"
          exit 1

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install npm dependencies
        run: npm ci

      - name: Build Tailwind CSS
        run: npm run build:css

      - name: Check if Tailwind CSS was updated
        run: |
          if [ -n "$(git status --porcelain static/css/tailwind.css)" ]; then
            echo "âš ï¸ Tailwind CSS file was modified during build (this is normal in CI/CD)"
            echo "   The updated file will be used for deployment."
            git diff --stat static/css/tailwind.css || true
          else
            echo "âœ… Tailwind CSS is up to date"
          fi

      - name: Deploy to server via SSH
        uses: appleboy/ssh-action@v1.2.4
        with:
          host: ${{ secrets.DEPLOY_HOST }}
          username: ${{ secrets.DEPLOY_USER }}
          key: ${{ secrets.DEPLOY_SSH_KEY }}
          port: ${{ secrets.DEPLOY_SSH_PORT || 22 }}
          script: |
            set -e
            echo "ðŸš€ Starting deployment..."

            # Navigate to project directory
            cd /opt/convertica

            # CRITICAL: Check if a newer tag exists before starting deployment
            # This prevents deploying outdated tags even if the build already started
            echo "ðŸ” Checking for newer tags..."
            CURRENT_TAG="${{ github.ref_name }}"
            echo "Current deploying tag: $CURRENT_TAG"

            # Export Sentry environment & release so docker-compose passes them into Django
            export SENTRY_ENVIRONMENT=production
            export SENTRY_RELEASE="convertica@${CURRENT_TAG}"
            echo "Using SENTRY_ENVIRONMENT=$SENTRY_ENVIRONMENT"
            echo "Using SENTRY_RELEASE=$SENTRY_RELEASE"

            # Fetch all tags from remote
            git fetch --tags --force origin

            # Get all version tags sorted by version
            ALL_TAGS=$(git tag -l "v*" | sort -V)

            # Find the latest tag
            LATEST_TAG=$(echo "$ALL_TAGS" | tail -1)
            echo "Latest tag in repository: $LATEST_TAG"

            # Compare versions using sort -V (version sort)
            if [ "$CURRENT_TAG" != "$LATEST_TAG" ]; then
              # Check if current tag is older than latest using version sort
              SORTED_TAGS=$(printf '%s\n' "$CURRENT_TAG" "$LATEST_TAG" | sort -V)
              FIRST_TAG=$(echo "$SORTED_TAGS" | head -1)

              if [ "$FIRST_TAG" != "$CURRENT_TAG" ]; then
                echo "âš ï¸  WARNING: Newer tag $LATEST_TAG exists!"
                echo "ðŸ›‘ Current tag $CURRENT_TAG is outdated, cancelling deployment"
                echo "   This prevents deploying an old version when a newer one is available"
                exit 1
              fi
            fi

            echo "âœ… Current tag $CURRENT_TAG is the latest, proceeding with deployment"

            # Save server-specific files (SSL, staticfiles, etc.) before pull
            echo "ðŸ’¾ Saving server-specific files..."
            git stash push -m "Server local changes: staticfiles, ssl, renew-ssl.sh" || true

            # Pull latest code from repository
            echo "ðŸ“¥ Pulling latest code..."
            git fetch origin
            git reset --hard origin/main

            # Note: Tailwind CSS will be built INSIDE the container after it starts
            # This ensures CSS is built with the correct code version and copied to staticfiles
            echo "â„¹ï¸  Tailwind CSS will be built inside container after startup (before collectstatic)"

            # Restore server-specific files (if any were stashed)
            # Use stash apply instead of pop to keep stash in case of conflicts
            echo "ðŸ“¦ Restoring server-specific files..."
            if git stash list | grep -q "Server local changes"; then
              # Try to apply stash, if conflict - abort and keep server files
              if ! git stash apply --index 2>/dev/null; then
                echo "âš ï¸  Conflict in stash, keeping server files and aborting stash apply"
                git reset --hard HEAD
                git stash drop || true
              else
                git stash drop || true
              fi
            fi

            # Pull latest Docker images (if using Docker Hub)
            echo "ðŸ³ Pulling latest Docker images..."
            docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml pull web celery celery-beat || true

            # Rebuild containers (to get latest code changes)
            echo "ðŸ”¨ Rebuilding containers..."
            docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml build web celery celery-beat

            # Rolling deployment with automatic rollback strategy
            echo "ðŸ”„ Starting rolling deployment with automatic rollback..."
            echo "ðŸ“Š Current memory usage: ~2.1GB limits, server has 2GB RAM + 2GB swap = 4GB total"
            echo "âœ… Deployment strategy: Sequential (old stops before new starts) - fits in resources"

            # Save current container IDs for rollback BEFORE starting new ones
            # Note: docker compose up -d will stop and remove old containers, so we save IDs first
            # We need to save both running and stopped containers
            OLD_WEB_ID=$(docker ps -q -f name=convertica_web | head -1)
            if [ -z "$OLD_WEB_ID" ]; then
              # If no running container, check for stopped one
              OLD_WEB_ID=$(docker ps -a -q -f name=convertica_web | head -1)
            fi
            OLD_CELERY_ID=$(docker ps -q -f name=convertica_celery | head -1)
            if [ -z "$OLD_CELERY_ID" ]; then
              OLD_CELERY_ID=$(docker ps -a -q -f name=convertica_celery | head -1)
            fi
            OLD_CELERY_BEAT_ID=$(docker ps -q -f name=convertica_celery_beat | head -1)
            if [ -z "$OLD_CELERY_BEAT_ID" ]; then
              OLD_CELERY_BEAT_ID=$(docker ps -a -q -f name=convertica_celery_beat | head -1)
            fi
            echo "ðŸ“‹ Saved old container IDs for rollback: web=$OLD_WEB_ID, celery=$OLD_CELERY_ID, beat=$OLD_CELERY_BEAT_ID"

            # Step 1: Start new web container and wait for healthcheck
            # docker compose up -d will stop old and start new (sequential, not parallel)
            echo "ðŸ“¦ Starting new web container (waiting for healthcheck)..."
            echo "   Note: Old container will be stopped before new one starts (sequential deployment)"
            if ! docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml up -d --no-deps --wait web; then
              echo "âŒ Failed to start new web container"
              echo "ðŸ”„ Rolling back - restoring old container..."
              if [ -n "$OLD_WEB_ID" ]; then
                echo "   Restoring old web container (ID: $OLD_WEB_ID)..."
                docker start "$OLD_WEB_ID" 2>/dev/null || true
              else
                echo "   âš ï¸ No old container found to restore"
              fi
              exit 1
            fi

            NEW_WEB_ID=$(docker ps -q -f name=convertica_web)

            # Wait for container to be fully ready and healthy
            echo "â³ Waiting for web container to be fully ready and healthy..."
            max_health_attempts=30
            health_attempt=0
            container_healthy=false
            while [ $health_attempt -lt $max_health_attempts ]; do
              # Check if container is healthy (Docker healthcheck)
              if docker inspect --format='{{.State.Health.Status}}' "$NEW_WEB_ID" 2>/dev/null | grep -q "healthy"; then
                echo "âœ… Container is healthy!"
                container_healthy=true
                break
              fi
              health_attempt=$((health_attempt + 1))
              echo "   Waiting for healthcheck... (${health_attempt}/${max_health_attempts})"
              sleep 2
            done

            if [ "$container_healthy" = false ]; then
              echo "âš ï¸ Container healthcheck not passed, but continuing with additional wait..."
            fi

            # Additional wait to ensure Django is fully ready to accept requests
            echo "â³ Additional wait to ensure Django is fully ready..."
            sleep 10

            # Step 2: Run migrations (container must be running)
            echo "ðŸ“Š Running migrations..."
            max_attempts=5
            attempt=0
            migration_success=false
            while [ $attempt -lt $max_attempts ]; do
              if docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T web python manage.py migrate --noinput; then
                echo "âœ… Migrations completed!"
                migration_success=true
                break
              fi
              attempt=$((attempt + 1))
              echo "   Retry ${attempt}/${max_attempts}..."
              sleep 3
            done

            if [ "$migration_success" = false ]; then
              echo "âŒ Migrations failed after ${max_attempts} attempts"
              echo "ðŸ”„ Rolling back - stopping new container, restoring old..."
              docker stop "$NEW_WEB_ID" 2>/dev/null || true
              docker rm "$NEW_WEB_ID" 2>/dev/null || true
              if [ -n "$OLD_WEB_ID" ]; then
                echo "   Restoring old web container (ID: $OLD_WEB_ID)..."
                docker start "$OLD_WEB_ID" 2>/dev/null || true
              else
                echo "   âš ï¸ No old container found to restore"
              fi
              exit 1
            fi

            # Step 3: Compile translations (CRITICAL - must succeed)
            echo "ðŸŒ Compiling translations..."
            if ! docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T web python manage.py compilemessages; then
              echo "âŒ Translation compilation failed"
              echo "ðŸ”„ Rolling back - stopping new container, restoring old..."
              docker stop "$NEW_WEB_ID" 2>/dev/null || true
              docker rm "$NEW_WEB_ID" 2>/dev/null || true
              if [ -n "$OLD_WEB_EXISTS" ]; then
                echo "   Restoring old web container..."
                docker start "$OLD_WEB_EXISTS" 2>/dev/null || true
              fi
              exit 1
            fi

            # Step 4: Build Tailwind CSS inside container (before collectstatic)
            echo "ðŸŽ¨ Building Tailwind CSS inside container..."
            # Install Node.js and npm in container if not present, then build CSS
            # Run as root to install packages, but build as appuser to match file ownership
            if ! docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T --user root web sh -c "
              if command -v npm > /dev/null 2>&1; then
                echo '   npm found, building CSS...'
                cd /app
                chown -R appuser:appuser /app/static /app/package.json /app/package-lock.json 2>/dev/null || true
                su appuser -s /bin/sh -c 'cd /app && npm ci --silent 2>&1 | grep -v npm\ WARN || npm install --silent 2>&1 | grep -v npm\ WARN || true'
                su appuser -s /bin/sh -c 'cd /app && npm run build:css 2>&1' || echo 'âš ï¸ CSS build failed'
                ls -lh /app/static/css/tailwind.css || echo 'âš ï¸ CSS file not found'
              else
                echo 'âš ï¸ npm not found in container, installing Node.js...'
                apt-get update -qq > /dev/null 2>&1
                apt-get install -y -qq curl > /dev/null 2>&1 || true
                curl -fsSL https://deb.nodesource.com/setup_20.x | bash - > /dev/null 2>&1 || true
                apt-get install -y -qq nodejs > /dev/null 2>&1 || echo 'âš ï¸ Node.js installation failed'
                if command -v npm > /dev/null 2>&1; then
                  echo '   npm installed, building CSS...'
                  cd /app
                  chown -R appuser:appuser /app/static /app/package.json /app/package-lock.json 2>/dev/null || true
                  su appuser -s /bin/sh -c 'cd /app && npm ci --silent 2>&1 | grep -v npm\ WARN || npm install --silent 2>&1 | grep -v npm\ WARN || true'
                  su appuser -s /bin/sh -c 'cd /app && npm run build:css 2>&1' || echo 'âš ï¸ CSS build failed'
                else
                  echo 'âš ï¸ npm still not available, CSS should be in repository'
                fi
              fi
            "; then
              echo "âš ï¸ Tailwind CSS build failed, but continuing with collectstatic..."
            fi

            # Step 5: Collect static files (clear old files first)
            echo "ðŸ“¦ Collecting static files..."
            if ! docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T web sh -c "python /app/clear_staticfiles.py || true && python manage.py collectstatic --noinput && python /app/create_manifest.py || true"; then
              echo "âŒ Static files collection failed"
              echo "ðŸ”„ Rolling back - stopping new container, restoring old..."
              docker stop "$NEW_WEB_ID" 2>/dev/null || true
              docker rm "$NEW_WEB_ID" 2>/dev/null || true
              if [ -n "$OLD_WEB_ID" ]; then
                echo "   Restoring old web container (ID: $OLD_WEB_ID)..."
                docker start "$OLD_WEB_ID" 2>/dev/null || true
              else
                echo "   âš ï¸ No old container found to restore"
              fi
              exit 1
            fi

            # Step 6: Clear Django cache (Redis) to ensure fresh content after deploy
            echo "ðŸ—‘ï¸ Clearing Django cache..."
            docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T web python -c "from django.core.cache import cache; cache.clear(); print('Cache cleared!')" || true

            # Step 7: Verify health endpoint responds (critical check)
            echo "ðŸ¥ Verifying health endpoint (critical check for rollback)..."
            max_attempts=10
            attempt=0
            health_check_passed=false
            while [ $attempt -lt $max_attempts ]; do
              if docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T web curl -f http://localhost:8000/health/ > /dev/null 2>&1; then
                echo "âœ… Health check passed!"
                health_check_passed=true
                break
              fi
              attempt=$((attempt + 1))
              echo "   Retry ${attempt}/${max_attempts}..."
              sleep 2
            done

            if [ "$health_check_passed" = false ]; then
              echo "âŒ Health check failed after ${max_attempts} attempts"
              echo "ðŸ”„ Rolling back - stopping new container, restoring old..."
              docker stop "$NEW_WEB_ID" 2>/dev/null || true
              docker rm "$NEW_WEB_ID" 2>/dev/null || true
              if [ -n "$OLD_WEB_ID" ]; then
                echo "   Restoring old web container (ID: $OLD_WEB_ID)..."
                docker start "$OLD_WEB_ID" 2>/dev/null || true
              else
                echo "   âš ï¸ No old container found to restore"
              fi
              exit 1
            fi

            # Give web container a moment to fully stabilize
            echo "â¸ï¸ Waiting 3 seconds for web container to stabilize..."
            sleep 3

            # Step 7: Restart other services (with rollback on failure)
            echo "ðŸ”„ Restarting background workers..."
            if ! docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml up -d --no-deps celery celery-beat; then
              echo "âš ï¸ Failed to restart celery services, but web is healthy - continuing..."
            fi

            # Step 8: Reload nginx gracefully (no downtime)
            echo "ðŸ”„ Reloading nginx (graceful reload)..."
            docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml exec -T nginx nginx -s reload 2>/dev/null || {
              echo "   Nginx reload failed, restarting..."
              docker compose -f docker-compose.yml -f ci/docker-compose.prod.yml restart nginx
            }

            # Step 9: Clean up old stopped containers (only if new ones are healthy)
            echo "ðŸ§¹ Cleaning up old stopped containers..."
            # Remove old stopped containers (docker compose may have left them)
            docker container prune -f --filter "name=convertica_web" --filter "status=exited" 2>/dev/null || true
            docker container prune -f --filter "name=convertica_celery" --filter "status=exited" 2>/dev/null || true
            docker container prune -f --filter "name=convertica_celery_beat" --filter "status=exited" 2>/dev/null || true

            echo "âœ… Deployment completed!"

            # Clean up old Docker images
            echo "ðŸ§¹ Cleaning up old Docker images..."
            docker image prune -f || true

            echo "âœ… Deployment completed!"
